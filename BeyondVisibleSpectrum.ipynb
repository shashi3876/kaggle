{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq/Hjyq9vAdy9CciUPWsLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashi3876/kaggle/blob/main/BeyondVisibleSpectrum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NxwQF3SviZjI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder = 'ot/ot/'\n",
        "npy_data = {}\n",
        "\n",
        "# Loop through all .npy files\n",
        "for filepath in glob.glob(os.path.join(folder, '*.npy')):\n",
        "    key = os.path.splitext(os.path.basename(filepath))[0]\n",
        "    try:\n",
        "        npy_data[key] = np.load(filepath)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {key} due to error: {e}\")\n",
        "\n",
        "# Optional: preview keys\n",
        "print(npy_data.keys())\n"
      ],
      "metadata": {
        "id": "I67stf_0iq2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_shape = (128, 128, 125)\n",
        "\n",
        "for key, arr in npy_data.items():\n",
        "    if arr.shape != expected_shape:\n",
        "        print(f\"File: {key}.npy --> Shape: {arr.shape}\")"
      ],
      "metadata": {
        "id": "_vsqo24sZamI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "vmiTGlnGZhHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test= pd.read_csv('test.csv')\n",
        "\n",
        "# Expected shape\n",
        "expected_shape = (128, 128, 125)\n",
        "\n",
        "# Step 1: Filter valid arrays in npy_data\n",
        "valid_npy_data = {\n",
        "    key: arr for key, arr in npy_data.items()\n",
        "    if isinstance(arr, np.ndarray) and arr.shape == expected_shape\n",
        "}\n",
        "\n",
        "# Step 2: Extract base filenames (without .npy) from train and test\n",
        "train['file_key'] = train['id'].str.replace('.npy', '', regex=False)\n",
        "test['file_key'] = test['id'].str.replace('.npy', '', regex=False)\n",
        "\n",
        "# Step 3: Keep only rows with corresponding valid arrays\n",
        "train_clean = train[train['file_key'].isin(valid_npy_data)].copy()\n",
        "test_clean = test[test['file_key'].isin(valid_npy_data)].copy()\n",
        "\n",
        "# Step 4: Prepare arrays for NN training\n",
        "X_train = np.stack([valid_npy_data[fid] for fid in train_clean['file_key']])\n",
        "y_train = train_clean['label'].values\n",
        "\n",
        "# Step 5: Prepare arrays for test set\n",
        "X_test = np.stack([valid_npy_data[fid] for fid in test_clean['file_key']])\n",
        "test_ids = test_clean['id'].values\n",
        "\n",
        "# Step 6: Create final test DataFrame\n",
        "test_df = pd.DataFrame({'id': test_ids})\n",
        "\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Target shape for CNN input\n",
        "target_shape = (64, 64, 64)\n",
        "\n",
        "def resize_volume(volume, target_shape):\n",
        "    return resize(volume, target_shape, mode='constant', preserve_range=True)\n",
        "\n",
        "# Apply resizing to all training and test volumes\n",
        "X_train_resized = np.array([resize_volume(arr, target_shape) for arr in X_train])\n",
        "X_test_resized = np.array([resize_volume(arr, target_shape) for arr in X_test])\n",
        "\n",
        "# Add channel dimension for CNN input\n",
        "X_train_cnn = X_train_resized[..., np.newaxis] / 255.0\n",
        "X_test_cnn = X_test_resized[..., np.newaxis] / 255.0\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Flatten the training data\n",
        "X_train_flat = X_train_cnn.reshape(X_train_cnn.shape[0], -1)\n",
        "\n",
        "# Fit PCA on the flattened training data\n",
        "pca = PCA()\n",
        "pca.fit(X_train_flat)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Determine number of components for each threshold\n",
        "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "n_components_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
        "\n",
        "print(f\"Components for 90% variance: {n_components_90}\")\n",
        "print(f\"Components for 95% variance: {n_components_95}\")\n",
        "print(f\"Components for 99% variance: {n_components_99}\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Flatten the 3D arrays\n",
        "X_train_flat = X_train_cnn.reshape(X_train_cnn.shape[0], -1)\n",
        "X_test_flat = X_test_cnn.reshape(X_test_cnn.shape[0], -1)\n",
        "\n",
        "# Step 2: Apply PCA with 337 components\n",
        "pca = PCA(n_components=337)\n",
        "X_train_pca = pca.fit_transform(X_train_flat)\n",
        "X_test_pca = pca.transform(X_test_flat)\n",
        "\n",
        "# Step 3: Fit Ridge regression\n",
        "reg = Ridge()\n",
        "reg.fit(X_train_pca, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred_pca = reg.predict(X_test_pca)\n",
        "\n",
        "# Step 5: Format submission\n",
        "submission_pca = pd.DataFrame({'id': test_df['id'], 'label': y_pred_pca})\n",
        "submission_pca['label'] = submission_pca['label'].clip(0, 100)\n",
        "submission_pca.to_csv('submission_pca.csv', index=False)\n",
        "\n",
        "# Ensure 'id' columns are strings for safe comparison\n",
        "submission_ids = submission_pca['id'].astype(str)\n",
        "test_ids = test['id'].astype(str)\n",
        "\n",
        "# Find missing ids from train\n",
        "missing_ids = test_ids[~test_ids.isin(submission_ids)]\n",
        "\n",
        "# Create new rows with label = 44\n",
        "new_rows = pd.DataFrame({'id': missing_ids, 'label': 49})\n",
        "\n",
        "# Append to submission_df\n",
        "submission_pca_extended = pd.concat([submission_pca, new_rows], ignore_index=True)\n",
        "\n",
        "# Optional: sort by id or reset index\n",
        "submission_pca_extended = submission_pca_extended.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "submission_pca_extended.rename(columns={'id': 'ID'}, inplace=True)\n",
        "submission_pca_extended.to_csv('submission_pca.csv', index=False)"
      ],
      "metadata": {
        "id": "bmiYz7kSZdWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}